{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddd7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f04c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 16s 541ms/step - loss: 0.9651 - accuracy: 0.7400 - val_loss: 0.7118 - val_accuracy: 0.5606\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 17s 604ms/step - loss: 0.5243 - accuracy: 0.7850 - val_loss: 0.5292 - val_accuracy: 0.7597\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 26s 944ms/step - loss: 0.4411 - accuracy: 0.8394 - val_loss: 0.5264 - val_accuracy: 0.7814\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 23s 808ms/step - loss: 0.4168 - accuracy: 0.8377 - val_loss: 0.4976 - val_accuracy: 0.8225\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 18s 642ms/step - loss: 0.3770 - accuracy: 0.8613 - val_loss: 0.5369 - val_accuracy: 0.7013\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 17s 621ms/step - loss: 0.3570 - accuracy: 0.8608 - val_loss: 0.5348 - val_accuracy: 0.6797\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 16s 567ms/step - loss: 0.3721 - accuracy: 0.8568 - val_loss: 0.5038 - val_accuracy: 0.7511\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 18s 634ms/step - loss: 0.3487 - accuracy: 0.8619 - val_loss: 0.4159 - val_accuracy: 0.8745\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 18s 638ms/step - loss: 0.3031 - accuracy: 0.8810 - val_loss: 0.5027 - val_accuracy: 0.7165\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 18s 622ms/step - loss: 0.3199 - accuracy: 0.8759 - val_loss: 0.3834 - val_accuracy: 0.8918\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 17s 601ms/step - loss: 0.3160 - accuracy: 0.8827 - val_loss: 0.3787 - val_accuracy: 0.8745\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 17s 618ms/step - loss: 0.3066 - accuracy: 0.8838 - val_loss: 0.3545 - val_accuracy: 0.8766\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 16s 556ms/step - loss: 0.3006 - accuracy: 0.8709 - val_loss: 0.3294 - val_accuracy: 0.8939\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 16s 565ms/step - loss: 0.3162 - accuracy: 0.8776 - val_loss: 0.4700 - val_accuracy: 0.7251\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 16s 554ms/step - loss: 0.3010 - accuracy: 0.8804 - val_loss: 0.2906 - val_accuracy: 0.9113\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 16s 559ms/step - loss: 0.3004 - accuracy: 0.8883 - val_loss: 0.2774 - val_accuracy: 0.9048\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 16s 561ms/step - loss: 0.2706 - accuracy: 0.8956 - val_loss: 0.2535 - val_accuracy: 0.9199\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 15s 545ms/step - loss: 0.2822 - accuracy: 0.8894 - val_loss: 0.2654 - val_accuracy: 0.9091\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 15s 541ms/step - loss: 0.2533 - accuracy: 0.8972 - val_loss: 0.2190 - val_accuracy: 0.9307\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 16s 568ms/step - loss: 0.2478 - accuracy: 0.9062 - val_loss: 0.1983 - val_accuracy: 0.9437\n",
      "INFO:tensorflow:Assets written to: gender_detection.model\\assets\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "learning_rate = 1e-3\n",
    "img_dims = (96,96,3)\n",
    "epochs = 100\n",
    "\n",
    "# loading image files from the dataset\n",
    "image_files = [f for f in glob.glob(r'C:\\Users\\Oggy\\Downloads\\Gender-Detection-master\\Gender-Detection-master\\gender_dataset_face' + \"/**/*\", recursive=True) if not os.path.isdir(f)]\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# converting images to arrays and labelling the categories\n",
    "for img in image_files:\n",
    "\n",
    "    image = cv2.imread(img)\n",
    "    \n",
    "    image = cv2.resize(image, (img_dims[0],img_dims[1]))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "\n",
    "    label = img.split(os.path.sep)[-2] # C:\\Files\\gender_dataset_face\\woman\\face_1162.jpg\n",
    "    if label == \"woman\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "        \n",
    "    labels.append([label]) # [[1], [0], [0], ...]\n",
    "\n",
    "# pre-processing\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# split dataset for training and validation\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "\n",
    "trainY = to_categorical(trainY, num_classes=2) # [[1, 0], [0, 1], [0, 1], ...]\n",
    "testY = to_categorical(testY, num_classes=2)\n",
    "\n",
    "# augmenting datset \n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# define model\n",
    "def build(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\": #Returns a string, either 'channels_first' or 'channels_last'\n",
    "        inputShape = (depth, height, width)\n",
    "        chanDim = 1\n",
    "    \n",
    "    # The axis that should be normalized, after a Conv2D layer with data_format=\"channels_first\", \n",
    "    # set axis=1 in BatchNormalization.\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# build model\n",
    "model = build(width=img_dims[0], height=img_dims[1], depth=img_dims[2],\n",
    "                            classes=2)\n",
    "\n",
    "# compile the model\n",
    "opt = Adam(learning_rate=learning_rate, decay=learning_rate/epochs)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=64),\n",
    "                        validation_data=(testX,testY),\n",
    "                        steps_per_epoch=len(trainX) // 64,\n",
    "                        epochs=epochs, verbose=1)\n",
    "\n",
    "# save the model to disk\n",
    "model.save('gender_detection.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e25176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
